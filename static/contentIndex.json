{"dev/Obsidian-vaultの一部のディレクトリだけをQuartzで公開する":{"slug":"dev/Obsidian-vaultの一部のディレクトリだけをQuartzで公開する","filePath":"dev/Obsidian vaultの一部のディレクトリだけをQuartzで公開する.md","title":"Obsidian vaultの一部のディレクトリだけをQuartzで公開する","links":[],"tags":["Obsidian"],"content":"何回目かわからないけどブログを作りました。\n試したい技術が現れるたびにブログを作っている気がしますが、今回はちゃんと更新していけるように頑張ります。\nこのブログは、Obsidianのvault（保管庫 ローカルのディレクトリ）の一部のディレクトリをブログ記事として切り出してSSGで記事化し公開する構成になっているので、その説明をします。\nQuartz自体の説明はここではしません。\nこの構成のメリット\npushごとに公開\nObsidianのノートを更新してpushするたびにデプロイjobが実行されるので公開が楽です。\nプライベートなvaultと同一リポジトリ内で独立\n通常、QuartzなどのSSGでブログを作るときは、SSGリポジトリの内部にObsidianのvaultを配置することになると思います。\nその場合、プライベートなvaultとは独立することになりますが、管理が面倒ですし、LLMの恩恵を受けながら記事を書きたい場合は同一のリポジトリで管理して、同じ場所でドキュメントをindex化して活用したいです。\nそういったことができるようになります。\n構成\nリポジトリは2つ登場します。\n\nobsidian-vault\n\nプライベートなObsidian vaultで普通にドキュメント管理として使う\n_published ディレクトリ内のドキュメントのみブログ記事としてビルドして公開したい\n\n\nobsidian-blog\n\nブログのSSGやUIなどを管理する（つまりこのブログの本体）\nソースコードはこちら\n\n\n\nアプローチは、obsidian-vaultの _published ディレクトリ内に変更があったときにGitHub Actionsでeventをdispatchしてobsidian-blog側のGitHub Actionsのトリガーにし、obdsidian-blogではobsidian-vaultをcloneしてきて _published 内のみSSGで記事を生成して公開するというものです。\nローカルでは obsidian-blog の content で obsidian-vault のシンボリックリンクを貼ればローカルでもプレビューできます。\nsequenceDiagram\n    participant OV as obsidian-vault\n    participant GA1 as GitHub Actions\n    participant GB as obsidian-blog\n    participant GA2 as GitHub Actions\n    participant GH as GitHub Pages\n\n    OV-&gt;&gt;GA1: _published/に差分あり\n    GA1-&gt;&gt;GB: vault-updated event\n    GB-&gt;&gt;GA2: triggered\n    GA2-&gt;&gt;OV: クローン\n    GA2-&gt;&gt;GA2: Quartz 4でビルド\n    GA2-&gt;&gt;GH: デプロイ\n\nDeploy用yml\nこれで _published に差分があったときに obsidian-blog のGAが受け取れるeventをdispatchします。\nDISPATCH_TOKEN は、obsidian-blogの contents:read,write 権限を持つ personal access token(PAT)です。\nobsidian-vault/.github/workflows/deploy.ymlname: Trigger Blog Deploy on Published Changes\n \non:\n  push:\n    branches:\n      - main\n    paths:\n      - &#039;_published/**/*.md&#039;\n \njobs:\n  dispatch:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Trigger deployment in obsidian-blog repo\n        uses: peter-evans/repository-dispatch@v3\n        with:\n          token: ${{ secrets.DISPATCH_TOKEN }}\n          repository: tachibanayu24/obsidian-blog\n          event-type: vault-updated\nこれで obsidian-vault から受け取ったeventをトリガーにしてデプロイを実行します。 obsidian-blog でmainブランチにpushしたときにも実行します。\nVAULT_ACCESS_TOKEN は、obsidian-vaultの contents:read 権限を持つ personal access token(PAT)です。\nobsidian-blog/.github/workflows/deploy.ymlname: Deploy Blog\n \non:\n  push:\n    branches:\n      - main\n \n  repository_dispatch:\n    types: [vault-updated] # obsidian-vaultの `_published` が更新されたらdispatchされる\n \npermissions:\n  contents: write\n \njobs:\n  build_and_deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout obsidian-blog Repo\n        uses: actions/checkout@v4\n        with:\n          path: obsidian-blog\n \n      - name: Checkout obsidian-vault Repo (to temp location)\n        uses: actions/checkout@v4\n        with:\n          repository: tachibanayu24/obsidian-vault\n          path: vault-temp\n          token: ${{ secrets.VAULT_ACCESS_TOKEN }}\n \n      - name: Prepare content directory\n        run: mkdir -p obsidian-blog/content\n \n      - name: Copy published content\n        run: |\n          if [ -d &quot;vault-temp/_published&quot; ] &amp;&amp; [ &quot;$(ls -A vault-temp/_published)&quot; ]; then\n            cp -r vault-temp/_published/* obsidian-blog/content/\n          else\n            echo &quot;Warning: vault-temp/_published directory is empty or does not exist.&quot;\n          fi\n \n      # attachmentsは通常 `_published` には配置しないので、画像など正しく表示するためにこれもコピーする\n      - name: Copy attachments to content root\n        run: |\n          if [ -d &quot;vault-temp/_config/attachment&quot; ] &amp;&amp; [ &quot;$(ls -A vault-temp/_config/attachment)&quot; ]; then\n            cp -r vault-temp/_config/attachment/* obsidian-blog/content/\n          else\n            echo &quot;Info: vault-temp/_config/attachment directory is empty or does not exist.&quot;\n          fi\n \n \n      - name: Setup Node, Install, Build\n        working-directory: obsidian-blog\n        env:\n          NODE_ENV: production\n        run: |\n          npm ci\n          npx quartz build\n \n      - name: Deploy to GitHub Pages\n        uses: peaceiris/actions-gh-pages@v3\n        with:\n          github_token: ${{ secrets.GITHUB_TOKEN }}\n          publish_dir: ./obsidian-blog/public\n          cname: blog.tachibanayu24.com\nおわり\nこれで単一vault内でプライベートな領域と公開用の領域で分けることができました。"},"dev/auto-article-publisher":{"slug":"dev/auto-article-publisher","filePath":"dev/auto-article-publisher.md","title":"全自動でブログの記事を生成して公開するようにしてみるテスト","links":[],"tags":["LLM","Firebase"],"content":"ここ数ヶ月はずっと、生成AI周りの活況や趨勢の変化がすごく、情報のキャッチアップがだいぶしんどいですよね。私は技術者ですが、役割上ビジネス的なユースケース・プロダクトにもついていかないといけないのでなかなか大変になってきました。\nそこで、平凡な発想ですが、信頼できる情報ソースと思うメディアのRSSを監視し、更新されたら自動でLMMに記事化させ、せっかくなのでこのブログにpublishするbotを実装して放流してみました。\n実際にbotにより投稿された記事は今のところ以下の２つです。\n\n【bot投稿🤖】OpenAIの新主力モデルGPT4.1 API提供開始 | ほこりログ\n【bot投稿🤖】最新AIニュース Veo 2とKling 2が一般公開 GPT-4.1も | ほこりログ\n\nそれなりに良い出来なのではないかと思っています。\n構成\nZapierやDifyなどワークフローを簡単に構成できるサービスを使いたかったのですが、使用できるファウンデーションモデルが限定的であったり、結局jsを実行するActionばかりになってしまい逆に面倒だったので、シンプルにサーバレス関数を実装しています。\nCloud SchedulerでCloud Run関数をトリガーしています。\n以下のように普通のことをやっています。\n\n{duration} 分に1度、関数をトリガーする\n事前に指定してあるすべてのRSSフィードをfetchしてparseする\n\n今回は簡易的に、pubDateが {duration} より新しいもののみ対象とする（なければ処理を終了）\n\n\n2で得た更新分のURLをfetchして、得られたHTMLから @mozilla/readability でコンテンツを抽出する\n3の結果と、過去の自分の記事、promptを与えてgemini 2.5 Proで記事のメイン部分とslugを生成させる\n4の結果にタグや作成日、slugなどを所定のフォーマットで保管する\n5で得た記事ファイル全体を、ブログのコンテンツを管理するprivate repositoryにpushする\n\npushでSSGのbuildを実行してdeployするようになっている(詳細)\n\n\n\n実装\nコードベースはこちらです。\nGitHub - tachibanayu24/rss-to-blog-publisher\nプロンプトはこまめに調整すると思うのでブログには書かないですが、現時点ではこの部分に記述しています。\n改善点\nやってみて思ったことですが、こういうユースケースでは、一度に必要なコンテキストをまとめて渡してほしいものを生成し切るようにするのではなく、もう少し分割的にLLMを呼び出したほうがいいかもしれないですね。\n今回の例では、「指示、過去記事（筆者の論調を真似るため）、記事の元ネタ」を渡して記事化までいっきにするのではなく、以下のようにステップが踏めたはずです。\n\n指示、記事の元ネタを渡して、まずはあまり情報を剥落させずにまとめさせる\n指示、過去記事を渡して、1の内容を筆者になりきって記事として仕上げさせる\n\n関数みたいに単一責任的に処理ステップを分割するということです。\nもちろん、関数がそうであるべき理由とは別の理由でです。LLMが進化して長いコンテキストを扱えるようになってきていますが、長過ぎるコンテキストを渡すとその中間部分はあまり使用されず、性能が劣化することはよく知られています。\n\n\nChanging the location of relevant information (in this case, the position of the passage that answers an input question) within the language model’s input context results in a U-shaped performance curve—models are better at using relevant information that occurs at the very beginning (primacy bias) or end of its input context (recency bias), and performance degrades significantly when models must access and use information located in the middle of its input context.\n\nLost in the Middle: How Language Models Use Long Contexts\n\n\nこのあたりもっと深堀りする価値があるなと思いました。別の記事でやるかも。\n今後の運用\nという感じで、1~2日に1回くらい、botが自動でAIのニュース（技術的関心が中心）を投稿するようになっています。\nただ、アウトプットの品質を見て、botの時点ではドラフト状態にして、人間が必ずレビューするようにしたりはするかもしれないです。ネットにゴミを増やすつもりは全然ないので。"},"dev/firebase-studio-testplay":{"slug":"dev/firebase-studio-testplay","filePath":"dev/firebase-studio-testplay.md","title":"ブラウザベースのAI統合開発環境 Firebase Studio を触ってみる","links":[],"tags":["Firebase","CodingAgent","LLM"],"content":"2025/4/9、Firebase Studioのプレビュー版がローンチされました。\n早速触ってみたので感想など書いていきます。\nFirebase Studioとは？\nIntroducing Firebase Studio で詳しく紹介されています。\n主な機能\n\n\n自然言語からのプロトタイプ作成\n\nプロンプト、画像、図面からネイティブアプリやWebアプリケーションを構築\nGemini APIキーは対話的に作成して設定できる\n\n\n\nAIチャットによる迅速な反復\n\nGemini in Firebaseとのチャットで、ユーザー認証の追加、レイアウト変更、UI改善などを実現\nコードベースを理解した上で自然言語の指示に基づいて更新\n\n\n\nCodeOSSベースのIDE\n\nコード補完、デバッグ、説明、ターミナルアクセスなどのGeminiコード支援\nFirebaseサービスとの統合\n\n\n\nマルチデバイスプレビュー\n\nWebプレビュー用の公開URL生成\nQRコードによるモバイルデバイスでのプレビュー\n\n\n\nワンクリックデプロイ\n\nFirebase App Hostingを活用した簡単デプロイ\nビルド、CDN、サーバーサイドレンダリングを自動処理\n\n\n\nリアルタイム共同作業\n\nワークスペース全体をURLで共有\n同じ環境内でのリアルタイム共同作業\n\n\n\n利用可能なプラン\n\n無料プラン: 3つのワークスペース\nGoogle Developer Program: 10のワークスペース\nPremium Google Developer Program: 30のワークスペース\n\n一部の統合（Firebase App Hostingなど）には課金アカウントが必要な場合があります。\n触ってみる\nApp Prototyping エージェントを使用してフルスタック ウェブアプリを開発、公開、モニタリングする  |  Firebase Studio にプロンプトが紹介されているので、これのとおりにプロトタイプを構築してみます。\n食品の画像を与えて、それがなんという食品なのかを判定し、レシピを教えてくれるアプリケーションですね。\n\n\n                  \n                  プロンプト \n                  \n                \n\nBuild a web app that can identify food products from an uploaded picture or\nin-browser camera. The app should provide a recipe that contains the\nidentified ingredients. 日本語で構築して\n\n\n完成品は以下のような感じです。\n\nとりあえずこれは一発でできました。\n初回の実行後には、自然言語で反復するのに特化したプロトタイパービューと、実際のコードを参照してデバッグできるコードビューで切り替えることができます。\n\nIDEの方はCodeOSSベースで、プレビューとGeminiとの会話が搭載されています。アンドロイドアプリもエミュレートしてこのようにプレビューできるらしいです。\n感想\n簡単なプロトタイピングはかなり楽ちんにできるので、特定のユースケースを検証するためにノーコードでアプリケーションを構築するとかのニーズは十分満たしそうに見えます。\nただし、Firebase Studioという命名とは裏腹に、FirestoreとかStorageをセットアップして接続して…とかはできないので、その部分は自分でやって上げる必要があるのがちょっと面倒な感じ。それ以外の品質とかは既存のコーディングエージェントと大差ない印象です。\n少し高度なことをやろうとするとエラーも多いです。stack traceとかみて修正を試みてはくれるんですが、そんなに解決しないですね…\n\n良かったところ\n\n3つまで無料\nブラウザベースのIDEがすぐに立ち上がるのでどの環境でも手軽に扱える\nGemini APIキーをボタン一つで作成して利用してくれる\nVueやNextなどテンプレートを選択できるので、簡単に技術スタックをざっくりと指定できる\nAI機能部分はgenkitを用いてワークフローを必ず構築するようになっているっぽい\n\n入出力のzodスキーマとかも\n\n\n\n\n今一つなところ\n\nアウトプットの精度に驚きは特に無い\n\n既存のコーディングエージェントと比べて優れているとは思わない\n\n\nFirestoreなど、Firebaseのサービスの構築もしてくれるとかではない\n\nFirestoreで実行履歴を永続化して、とかの指示をすると、コードはそれっぽくやってくれるが各サービスを有効化したりkeyを自動で入力してくれたりとかはまだない感じ\nhostingで公開するくらい\n\n\n\n\n"},"dev/vibe-coding-with-gpt-4_1":{"slug":"dev/vibe-coding-with-gpt-4_1","filePath":"dev/vibe-coding-with-gpt-4_1.md","title":"GPT-4.1をCursorのAgent Modeで使ってみる","links":[],"tags":["CodingAgent","LLM"],"content":"OpenAIがGPT-4.1をリリースし、すぐさまCursorは「肌感を掴んでもらうために当面は無料で提供します」と利用可能のモデルにこれを追加しました。(Xのポストを参照)\nOpenAIの発表によると、コーディング能力が向上したり長文コンテキストの処理能力や理解力が改善したということです。さっそくCursorで触ってみます。\nVibe codingやるぞ\n対話1: とりあえず作り切らせる\nごく簡単なCRUDを持つアプリケーションを要求します。今回作るのは旅行とかで作成する持ち物リストを管理するためのアプリケーションです。\n以下のプロンプトから開始します。\nプロンプト旅行の持ち物リストを管理するアプリケーションを作ってみて。とりあえず最後まで作り切ってみて。\n  \n- ログイン機能は不要  \n- ユーザーは持ち物の追加、削除、編集ができる  \n- データは永続化されるが、デモアプリなので簡易的な手段を選択して良い（SQLiteなど）  \n- テンプレートを追加できる（財布など行き先に関係なく必ず持っていくようなものをまとめることができる）  \n- カバンに入れたかどうかの状態をcheckboxで管理できる  \n- おしゃれでモダンなUI\nで、特に躓くことなくここまではコーディングしてくれました。\n\nバックエンドはexpress, sqlite3、フロントエンドはvite, typescript, react, emotion, muiというスタックを選択したようです。\nnpm create vite@latest . -- --template react-ts でプロジェクトを開始した際に作成された index.css により color-schema が dark になっており、そのせいで見づらいようです。\nしかしちゃんとデータの永続化はできていますね。（画像の入力値はこちらで入れたもの）\nプロンプト&gt; すべてのテーブルの中身を標準出力して\n \n全テーブル（items, templates, template_items）の中身は以下の通りです。\n \n---\n \n### items テーブル\n \n| id | name | packed |\n|----|-----------|--------|\n| 3 | test | 0 |\n| 4 | hogehoge | 0 |\n \n### templates テーブル\n \n| id | name |\n|----|----------------|\n| 1 | testとhogehoge |\n \n### template_items テーブル\n \n| template_id | name | packed |\n|-------------|-----------|--------|\n| 1 | test | 0 |\n| 1 | hogehoge | 0 |\n \n対話2: ブラッシュアップする\nプロンプトちゃんと動いているようです。以下についてブラッシュアップしてください。  \n  \n- create viteしたときに生成された余計なファイルが残っています\n\t- 不要なものは消してください\n\t- 特に、index.cssが影響して、OSがダークモードのユーザーは視認性がとても悪いです  \n- UIがダサすぎるのでもっとモダンなwebサービスっぽくして  \n- タイトルをもっといい感じに考えて\n結果が以下の通り。割と良いのではないでしょうか？\n\n対話3: ちょっとだけ難しそうなことを言ってみる\n限定的なコンテキストしか持たないのでしょうがないんですが、他のシステムに依存するとうまくいかないことが多いので、geminiのAPIを利用するように促してみます。\nプロンプトbackend/.env に、 GEMINI_API_KEY という命名でgeminiのAPIキーを追加しました。  \nこれで、行き先と泊数を入力したら、持ち物リストを提案してくれて、それを編集して適用できるような機能を追加してください。\n結果は以下。一発では無理だったのでここからデバッグさせます。\n\nデバッグのやり取りは何往復もしたので載せませんが。。\n流れとしては、\n\nbackendの必要な依存関係をインストールしていなかった\ngeminiの存在しないモデルを指定していた\ngeminiに与えるプロンプトがおかしかった\n\nという感じでした。2の解決は結局できず、人間が修正しました。\n対話3の完成形は以下のとおりです。良いのではないでしょうか。\n　\nまだClaudeやgeminiの最上位と比較してどうかは評価しきれませんが、そんなに悪くなさそうなのでちょっと使ってみようと思います。\n最終形のコードベースはこちらです。\nGitHub - tachibanayu24/gpt-4.1-test\n（おまけ）うざかったところ\n今回 cursor ruleとか特に設定せずに素手でやったので、ちゃんとやれば防げるんでしょうけど。\n\n人間が修正したところを何回も戻そうとする\nすぐ迷子になって cd: no such file or directory\nnodeをkillすな！\n\n"},"finance/2025-04-06-米国株の現在地の確認":{"slug":"finance/2025-04-06-米国株の現在地の確認","filePath":"finance/2025-04-06 米国株の現在地の確認.md","title":"2025-04-06 米国株の現在地の確認","links":[],"tags":["株式"],"content":"値動きが激しいので現在の米国株（S&amp;P500）の状況を確認します。\nあんまりこの手の話題で自分の意見を書くのもどうかなと思うので、客観的な事実だけをまとめます。（市場の予想は一切しません）\n最近の主な出来事\nトランプ氏、相互関税を発表　最大50％で日本は24％ - BBCニュース\n\n\n                  \n                  AIによる要約 \n                  \n                \n\nトランプ米大統領が全輸入品への新関税計画を発表。10%基本税に加え、「最悪の違反者」に追加関税。日本には24%が課される。中国やEUにも高関税。「経済的独立宣言」とし、世界経済への影響が懸念されている。\n\n\n中国が報復関税を発表、米株式市場はパンデミック以来の大幅安に - BBCニュース\n\n\n                  \n                  AIによる要約 \n                  \n                \n\n中国が米国の関税に対抗し、全米輸入品へ34%の報復関税を発表。これを受け欧米株価は急落、米主要指数は5%超下落しパンデミック以来最悪の週となった。貿易戦争激化と世界経済への悪影響が懸念される。\n\n\nチャート\nS&amp;P500の直近高値は6147USD、現在の株価は5074USDです。\n2/19の高値から1ヶ月半ほどしか経過していませんが、17.5%ほど下がっています。\n調整局面の目安は10%、弱気相場の目安は20%なので、今は調整局面です。\nちなみにナスダックとラッセル2000はすでに弱気相場入りしています。\n月足では3月に -5.75% と大きく下落しましたが、現時点で4月もすでに -9.58% 下げています。\n\n日足では200日移動平均線で弾かれたあとに、2日連続で出来高を伴って窓を開けて大きく下落しています。\n\n金曜日は広範囲の銘柄が売られたのでヒートマップは真っ赤になっています。\n\n出典: FINANCIAL VISUALIZATIONS\nセンチメント\n恐怖指数(VIX)\nVIXはこの急落を受けて45.3まで急騰しました。\n直近でVIXが40を超えたときのイベントは以下のとおりです。\n\n2024年8月\n\n日経平均株価4451円安　下げ幅ブラックマンデー超え最大 - 日本経済新聞\n\n\n2020年3月〜4月\n\nコロナウイルスの世界的流行\n\n\n\nつまり、一瞬のパニック売があった2024年8月を除けばコロナウイルス流行時以来の水準です。\n\nFear &amp; Greed Index\n現在値は 4 で Extreme Fearです。\n\n出典: CNN Business\nプットコールレシオ\nプットオプションとコールオプションの比で表されるプットコールレシオは現在 1.07です。（IBD）\n1を超えているので、弱気が優勢になってきているということが言えます。\n来週のイベント\n\n4/9\n\nデルタ航空（DAL）決算\n\n\n4/10\n\n米国 消費者物価指数（CPI・コアCPI）\n米国 新規失業保険申請件数\n\n\n4/11\n\n米国 生産者物価指数（PPI）\n米国 ミシガン大学消費者信頼感指数\nJPモルガン（JPM）決算\nウェルズ・ファーゴ（WFC）決算\n\n\n\n再来週（4/14~）は小売売上高、鉱工業生産指数やゴールドマン・サックス、バンク・オブ・アメリカ、ユナイテッドヘルス、ネットフリックスなどの決算があります。\n各国の米国関税に対するリアクションもどんどん出てくるかもしれません。"},"index":{"slug":"index","filePath":"index.md","title":"TOP","links":["tags/自動生成記事"],"tags":[],"content":"たちばなゆうとのブログです。\n主にソフトウェア、LLM、スタートアップビジネス、ファイナンスなどの話題の記事を書くつもりです。\n自動生成記事  タグが付いているものは、botにより自動で生成・投稿された記事です。\n"},"misc/chiipoke-number-scale":{"slug":"misc/chiipoke-number-scale","filePath":"misc/chiipoke-number-scale.md","title":"ちいぽけにおける数字の扱い方に関する考察","links":[],"tags":["数学","雑学"],"content":"ちいかわぽけっと（ちいぽけ）がサイバーエージェントグループの applibot社からリリースされました。\nスマホのゲームはあんまりやらないのですが、初めてのちいかわのちゃんとしたスマホゲームということでちょっとプレイしています。数字の扱い方がヤバすぎるのでどうなっているのか少し考えてみようと思います。\n単位の表記\n\n出典: ちいかわぽけっと\n画面上に o とか h とかのアルファベットがありますが、これは桁数を表現しています。\nちいぽけにおける所持金や攻撃力、体力などのパラメータは非線形に急激に増加していき、 k = 10^3 とかではなく、 a = 10^3, b=10^6, c=10^9, ... といった具合にオーダーが増加していきます。\nこれを一般項にすると以下のように言えます。\n\nS: abなどのアルファベット\nn(S): aから数えたときのアルファベットの登場順\nL:  Sの文字数\ns_i: Sの左から i 番目の文字(abcのとき、 s_2 は b)\nval(s_i): 文字のインデックス(a=0, b=1, …, z=25)\n\nとすると、\nアルファベットS が何番目の記号かを示す n(S)\nn(S) = \\left( \\sum_{k=1}^{L-1} 26^k \\right) + \\left( \\sum_{i=1}^{L} \\mathrm{val}(s_i) \\times 26^{L-i} \\right) + 1\n一般的な表現 V\n\\mathrm{V} = X \\times 10^{3 \\times n(S)}\n現時点で確認されているのは aa とアルファベット二桁までなので、最低限ここまではあるだろうと思われるzzをこれに適用します。\nS = zz , L = 2 , s_1 = z , s_2 = z とすると、zzは702番目の記号とわかります。\nn(zz) = \\left( \\sum_{k=1}^{2-1} 26^k \\right) + \\left( \\sum_{i=1}^{2} val(s_i) \\times 26^{2-i} \\right) + 1\n= \\left( 26^1 \\right) + \\left( val(s_1) \\times 26^{2-1} + val(s_2) \\times 26^{2-2} \\right) + 1\n= 26 + (25 \\times 26 + 25 \\times 1) + 1\n= 702\n702番目の記号zzは、 3×n(zz)=3×702=2106 より 10^{2106} であるとわかります。天文学的という形容では全く追いつかないくらい大きな値です。\n論理的な表現\n一般的な64ビットの倍精度浮動小数点数（ double 型）では、大体  10^{\\pm308} くらいの非常に大きな（or 小さな）数を表現できますが、ちいぽけには足りません。\nしかしちいぽけはゲーム設計上、非常に大きな値と相対的に小さな値の演算は重要ではないはずなので数字の精度はざっくりで良いはずです。\nどんなふうに扱っているのかを想像してみます。\n浮動小数点数の応用を考えてみます。浮動小数点数では、数を「仮数部（有効桁数を持つ部分）」と「指数部（10の何乗かを示す部分）」に分けて表現します。\nこの構造を拡張して考えるとさらに大きな値も扱うことができそうです。ここで仮数部と指数部を以下のように定義してみます。\n\n仮数部: ゲームプレイに必要な精度のみ保持する\n\ne.g. 5.25 のような2桁程度\n\n\n指数部: 「10の何乗か」を示す部分\n\n整数型など\n\n\n\nこうすると以下のようになります。\n\n5.2aa (5.2×10^81) は、内部的に { 仮数部: 5.2, 指数部: 81 }\n1.0zz (1.0×10^2106) は、内部的に { 仮数部: 1.0, 指数部: 2106 }\n\nといった形でデータを保持できるはずです。\njavascriptで表現してみます。負の値は今のところ見たことがないので考慮しません。\nclass ChiipokeNumber {\n  /**\n   * @param {number} mantissa 仮数部\n   * @param {number} exponent 指数部\n   */\n  constructor(mantissa = 0, exponent = 0) {\n    this.mantissa = Number(mantissa);\n    this.exponent = Number(exponent);\n    this.normalize();\n  }\n \n  /**\n   * 数値を正規化する（仮数部が1以上10未満になるようにする）\n   * @method normalize\n   * @returns {void}\n   */\n  normalize() {\n    if (this.mantissa === 0) {\n      this.exponent = 0;\n      return;\n    }\n    while (this.mantissa &gt;= 10) {\n      this.mantissa /= 10;\n      this.exponent++;\n    }\n    while (this.mantissa &lt; 1 &amp;&amp; this.mantissa !== 0) {\n      this.mantissa *= 10;\n      this.exponent--;\n    }\n  }\n \n  /**\n   * @param {ChiipokeNumber} num\n   */\n  multiply(num) {\n    const newMantissa = this.mantissa * num.mantissa;\n    const newExponent = this.exponent + num.exponent;\n    return new ChiipokeNumber(newMantissa, newExponent);\n  }\n \n  /**\n   * @param {ChiipokeNumber} num\n   */\n  add(num) {\n    // 片方の値が相対的に非常に大きい場合は小さい方を無視\n    if (Math.abs(this.exponent - num.exponent) &gt; 16) {\n      return this.exponent &gt; num.exponent ? new ChiipokeNumber(this.mantissa, this.exponent) : new ChiipokeNumber(num.mantissa, num.exponent);\n    }\n  \n    let newMantissa;\n    let newExponent;\n    if (this.exponent &gt;= num.exponent) {\n      newExponent = this.exponent;\n      newMantissa = this.mantissa + num.mantissa / (10**(this.exponent - num.exponent));\n    } else {\n      newExponent = num.exponent;\n      newMantissa = num.mantissa + this.mantissa / (10**(num.exponent - this.exponent));\n    }\n    return new ChiipokeNumber(newMantissa, newExponent);\n  }\n \n  /**\n   * アルファベット表記を得る\n   * @method getNotationString\n   * @param {number} n   単位インデックス (a=1, aa=27, ...)\n   * @returns {string} アルファベット表記\n   */\n  static getNotationString(n) {\n    // 0以下の場合はアルファベットなし\n    if (n &lt;= 0) return &quot;&quot;;\n \n    const alphabet = &quot;abcdefghijklmnopqrstuvwxyz&quot;;\n    let L = 0; // アルファベットの文字数\n    let sumPowers = 0; // L-1 文字以下の単位の総数\n    let currentPower = 1; // 基数\n \n    while (true) {\n        const unitsInThisLength = currentPower * 26;\n        if (n &lt;= sumPowers + unitsInThisLength) {\n            L++;\n            break;\n        }\n        L++;\n        sumPowers += unitsInThisLength;\n        currentPower *= 26;\n \n        // アルファベット10文字数を超えるとエラーにする（現状確認されているのは2までだが）\n        if (L &gt; 10) throw new Error(&quot;L is too large&quot;); \n    }\n \n    const nPrime = n - sumPowers - 1; // 長さL内での0基準インデックス\n \n    let notation = &quot;&quot;;\n    let tempNPrime = nPrime;\n    for (let i = 0; i &lt; L; i++) {\n        const power = 26**(L - 1 - i);\n        const index = Math.floor(tempNPrime / power);\n        if (index &lt; 0 || index &gt;= 26) throw new Error(&quot;index is out of range&quot;);\n        notation += alphabet[index];\n        tempNPrime %= power;\n    }\n    return notation;\n  }\n \n  /**\n   * ゲーム内表記文字列を返す\n   * @method toDisplayNumber\n   * @returns {string} ゲーム内表記文字列\n   */\n  toDisplayNumber() {\n    if (this.exponent &lt; 3) {\n      const value = this.mantissa * (10**this.exponent);\n      const fixedValue = value.toFixed(2);\n      const displayValue = fixedValue.endsWith(&#039;.00&#039;) ? fixedValue.slice(0, -3) : fixedValue;\n      return displayValue;\n    }\n \n    const notationIndex = Math.floor(this.exponent / 3);\n    const remainderExponent = this.exponent % 3;\n    const displayMantissa = this.mantissa * (10**remainderExponent);\n    const notationString = ChiipokeNumber.getNotationString(notationIndex);\n \n    return `${displayMantissa.toFixed(2)}${notationString}`;\n  }\n}\n使用例\n// 足し算\nconst a = new ChiipokeNumber(5.2, 20); \nconst b = new ChiipokeNumber(1.5, 21);\n \na.toDisplayNumber() \n// -&gt; &#039;520.00f&#039;\nb.toDisplayNumber() \n// -&gt; &#039;1.50g&#039;\na.add(b).toDisplayNumber() \n// -&gt; &#039;2.02g&#039;\n \n// 掛け算\nconst a = new ChiipokeNumber(5, 35); \nconst _15 = new ChiipokeNumber(15, 0);\n \na.toDisplayNumber() \n// -&gt; &#039;500.00k&#039;\na.multiply(_15).toDisplayNumber() \n// -&gt; &#039;7.50l&#039;\n \n// 超巨大な値同士の掛け算\nconst a = new ChiipokeNumber(123.456, 1234); \nconst b = new ChiipokeNumber(456.789, 5678);\n \na.toDisplayNumber()\n// -&gt; &#039;1.23ov&#039;\nb.toDisplayNumber()\n// -&gt; &#039;45.68btu&#039;\na.multiply(b).toDisplayNumber()\n// -&gt; &#039;56.39cjq&#039;\n実際のロジックはわかりませんが、このようにしてゲーム上のパラメータというユースケースにおいては非常に巨大な値を扱うことができそうです。\nおわりに\nちいぽけのゲーム、賛否両論かなりあるけど個人的にはちいかわのゲームっぽくて好きです。みんなでプレイしよう！"},"misc/obsidian-for-llm":{"slug":"misc/obsidian-for-llm","filePath":"misc/obsidian-for-llm.md","title":"LLMとなかよくするためにObsidianを使う","links":["dev/Obsidian-vaultの一部のディレクトリだけをQuartzで公開する"],"tags":["Obsidian","LLM"],"content":"Obsidianは、ローカルのMarkdownファイルを知識ベースとして活用できるノートアプリです。OSSではないですが、コミュニティによるプラグインの開発が活発で色々とカスタマイズできます。\n「ローカルにすべてある」、「拡張可能性が高い」というところがLLMと組み合わせてテキストであれこれするのにとても相性がいいと思い使い始めました。\nまだ使い始めて日が浅いんですが、今のところうまいことワークしており、NotebookLMとかはもはや使わなくなってしまいました。Obsidian x LLM周りのやってることを紹介したいと思います。\n考え方\nObsidianは普通デイリーノートを取ったり日々のアイデアを書き留めてノート同士をリンクし、非線形な知識の構造化みたいなことを目指すのだと思いますが、私は自分だけが参照するために文章を書く習慣があまりないです。\nObsidianにはとにかく情報をぶち込みまくって、LLMの補助を受けながら解釈してアウトプットすることを目的に利用しています。\nなのでグラフビューとか全く使ってないです。（たぶん邪道ではある、が満足してる）\n情報収集: 「後で読む」にぶち込むと同時にAI要約する\nObsidian Web ClipperというChrome拡張で開いているwebページのコンテンツをObsidianにノートとしてぶち込むことができます。\nそのまま使っても割といい感じにメインのコンテンツだけ切り出してmarkdown化してくれますが、Web Clipperには Interpreter 機能があり、これを使うとノートを作成する前にLLMによる処理を挟むことができます。\n例えばこんな感じで記事を要約し、この記事を読んで取るべきNextActionを挙げてくれます。\n\n（青っぽいcallout部分がAIが生成したテキストです）\n後で読もうと思った記事が新規タブに溜まりがちでしたが、とりあえずweb clipperで要約付きのノートを作成しておいて、後でまとめて目を通すようになりました。\n後で読むリストは、優先度と読んだフラグ付きで一覧化され、優先度の降順で表示されるようにしています。\n\nweb clipperの設定\nObsidian Web Clipperから拡張機能をインストールし、拡張機能の settings &gt; Interpreter で任意のモデルを追加してAPIキーを入力します。\nその後、 New template からテンプレートを作成し、Note contentを以下のようにします。\n&gt; [!abstract] Interpreter Note\n&gt; \n&gt; {{&quot;コンテンツを日本語で要約してください。\\n- **共通ルール**\\n  - 読者はベンチャーキャピタリストかつソフトウェア開発者\\n  - 空白行やheading含むすべての行は `&gt; ` で開始して引用であることを示す\\n  - listで句点を利用せず、補足はインデントを付ける\\n- **フォーマット**\\n&gt; ## Summary\\n最大10個のbullet list(`- `)で箇条書きにより要約する。 \\n&gt; ## NextAction\\n最大で3個のcheckbox(`- [ ] ` で出力する。&quot;}}\n\n{{content}}\n\nスニペットは改行できなかったので \\n で改行を表現している感じです。 {{content}} に実際に保存されるコンテンツがmarkdownで入るので、要約以外にも色々やれます。\nリスト化の設定\nDataviewプラグインを使います。プラグインをインストールし、 _index などリストを表示したいページに以下のようにfrontmatter yamlとdataviewで実行したいjsのコードを書きます。\n---\nsearch: \ncssclasses:\n  - table-wide\n  - table-nowrap\n  - table-tiny\n  - row-alt\n---\n\ndataviewに実行させるには、jsの部分は以下のようにcodeblockで入力する必要があります。\n```dataviewjs\nconst MAX = 64000\n \nconst q = dv.current().search || &quot;&quot;\nconst s = dv.current().file.folder\n \nconst d = dv.pages(`&quot;${s}&quot;`).file\n  .filter(x =&gt; x.name.includes(q))\n  .filter(x =&gt; x.name !== &quot;_Index&quot;)\n  .sort(x =&gt; x.mtime, &quot;desc&quot;)\n  .sort(x =&gt; {\n    const priority = x.frontmatter.priority;\n    if (priority === undefined || priority === null) return 1;\n    const numPriority = Number(priority);\n    return isNaN(numPriority) ? 1 : -numPriority;\n  })\n  .sort(x =&gt; x.frontmatter.read ? 1 : 0)\n  .limit(100)\n  .map(x =&gt; {\n  return [\n    x.frontmatter.read ? &#039;✅&#039; : &#039;&#039;, \n    x.frontmatter.priority,\n    x.link, \n    `&lt;progress value=${x.size/MAX}&gt;&lt;/progress&gt;`,\n    x.mday.toLocaleString()\n  ]})\n  dv.table([&quot;read&quot;, &quot;priority&quot;, &quot;&quot;, &quot;size&quot;, &quot;updated&quot;], d)\n`\\`` &lt;- バッククオートはエスケープです。消してください！\nこの例では日付で降順にしたあとpriorityでまた降順にし、readが true になっているものを最後に持っていっています。jsなので好きに実装可能です。\n情報へのアクセス: CursorとObsidian Copilot\nObsidianのvaultはCursorなどのエディタで開けば単なるmarkdownファイルなので、Q&amp;Aやプランニングなど、様々なことに活用できます。\n例えば筋トレの計画を立てるときは、YouTubeでもブログでもなんでもいいのでweb clipperで参照できる情報をどんどん突っ込んで、LLMといっしょに目標達成の道筋を考えることができます。\n.cursor/rules にディレクトリ構造など明示して育てていくのが良いと思います。\n一応 Obsidian Copilot というプラグインもあり、vaultをindex化してchatしたり、事前に設定した機能（選択範囲の翻訳など）にインスタントにアクセスすることもできます。こんな感じに。\n\nまあCursorで代替できるのであまり使ってないです。\n設定手順は Obsidian Copilotのすゝめ：ノート活動を変えるかもしれない壁打ち相手 - Qiita で詳しく解説されています。\nアウトプット: CursorとObsidianでブログを書く\nこのブログはObsidianで書いています。詳しい構築方法は Obsidian vaultの一部のディレクトリだけをQuartzで公開する で紹介しています。\n上記までのプライベートな用途のvaultと同じvault内でブログ記事も管理しており、そうすることでLLMで相互に参照しやすくしています。\n現状は、Cursorで記事の一部を書いてもらったり、記事のレビューをしてもらったりしているくらいですが、将来的には人間はレビューして承認するだけみたいな運用にしても良さそうです。（もちろん、ネットの海にゴミを増やすようなマネはしないようにケアしつつ）\nまとめ\nという感じに、情報のインプットからアウトプットまでをObsidianをハブにして運用しつつ、ほんのりLLMのサポートを受けているという感じです。\nNotion MCPが公式から公開されましたが、このユースケースは代替できないと思います。（APIをガンガン叩かないといけなかったりなど）\nなので移行とかはせずにどっちも使っていきます。"},"news/ai_news_veo2_kling2":{"slug":"news/ai_news_veo2_kling2","filePath":"news/ai_news_veo2_kling2.md","title":"【bot投稿🤖】最新AIニュース Veo 2とKling 2が一般公開 GPT-4.1も","links":[],"tags":["自動生成記事","LMM"],"content":"\n\n                  \n                  Warning\n                  \n                \n\nこの記事は、以下の情報源を参照し、LLMにより自動で生成・投稿された記事です。\n\nAI News ButtonDown\n\n内容の正確性にご注意ください。\n\n\n最新AIニュースまとめ Veo 2とKling 2が一般公開\nAIの世界は相変わらず動きが早いですね。今回は特に動画生成モデルの分野で大きな進展がありました。Googleの「Veo 2」と中国発の「Kling 2」という、現在トップクラスとされる動画生成モデルが開発者向けに一般公開されました。\nその他にも、OpenAIが新しい「GPT-4.1」ファミリーを発表したり、各種開発ツールやコミュニティで活発な動きが見られたり、AI研究の最前線からの興味深い報告があったりと、盛りだくさんです。さっそく詳しく見ていきましょう。\n動画生成モデルの進化が止まらない Veo 2 と Kling 2\nこれまでAI Newsではテキストやコーディング関連の話題が中心でしたが、今回は動画生成モデルの大きなニュースを取り上げます。\nArtificial Analysisの動画生成モデルリーダーボードでトップを争う2つのモデルが、ほぼ同時に開発者向けにAPIアクセスを開放しました。これは動画生成技術の現状を知る良い機会ですね。\nGoogle Veo 2\nGoogleの「Veo 2」は、Gemini API と Gemini Advanced/Whisk を通じて利用可能になりました。（以前はFal.ai経由での提供でした）\n注目すべきはその価格で、生成される動画1秒あたり35セントと、かなり手頃になっています。（ただし、実際の利用感とは異なる可能性もあるようです）\n生成される動画の品質も向上しており、物理法則への暗黙的な理解が素晴らしいとの声も上がっています。\n\nKuaishou Kling 2\n中国の快手（Kuaishou）が開発した「Kling 2」も同日に発表されました。\n価格は10秒のクリップで約2ドルとVeo 2より高価ですが、生成される動画の品質は非常に高いと評判です。ただし、利用には最低でも月額700ドル（3ヶ月契約）のパッケージ購入が必要となるようです。\n\nどちらのモデルも、テキストから高品質な動画を生成できる能力を示しており、今後のクリエイティブ分野での活用が期待されます。\nOpenAIから「GPT-4.1」ファミリーが登場\nOpenAIも負けじと新しいモデルファミリー「GPT-4.1」を発表しました。(OpenAIのアナウンス)\nAPI限定リリースとモデルラインナップ\n今回のリリースはAPI限定で、以下の3つのモデルが含まれます。\n\nGPT-4.1\nGPT-4.1 mini\nGPT-4.1 nano\n\nOpenAI Devsのポストによると、これらのモデルはAPI専用であり、既存のGPT-4.5 Previewは3ヶ月後の7月14日に廃止される予定です。GPT-4.1が同等以上の性能を低遅延・低コストで提供できるためとのこと。\n性能向上と特徴\nOpenAIの発表や開発者の声によると、以下の点が改善されています。\n\nコーディング能力の向上\n\nGPT-4.1はSWE-Bench Verifiedで54-55%という高いスコアを達成（Reasoningモデルではないにも関わらず）\n内部ベンチマークではGPT-4o比で60%改善（不要なファイル読み取り40%減、変更70%減、冗長性50%減）という報告も\n\n\n指示追従性の改善\n長文コンテキスト処理能力の向上\n\n最大100万トークンに対応\n\n\nコスト削減\n\nGPT-4oと比較して26%安価\n\n\n\n評価とベンチマーク\n一方で、Scaling01氏のように、API版のGPT-4.1はOpenRouterのプレビュー版（Quasar Alpha, Optimus Alpha）よりも性能が低い、mini版は他の多くのモデルよりスコアが低い、といった指摘もあります。また、コーディング性能では依然としてDeepSeekV3に劣るものの、価格は8倍という比較も。\nしかし、skirano氏は、GPT-4.1がベンチマークスコアだけでなく、**現実世界のタスク（特にフロントエンド開発やWebサイト構築）**に最適化されている可能性があると指摘しています。OpenAIのSam Altman氏も、ベンチマークは強力だが、現実世界での実用性に焦点を当てたと述べています。\nまた、Aidan Clark氏は「名付けは下手だけど、miniと付くモデルは🔥だよ」とコメントしており、miniモデルの性能にも期待が持てそうです。DiscordのLMArenaコミュニティでも、GPT-4.1 miniがGPQAベンチマークでフルバージョンに匹敵する結果を出したという観察が共有されています。\n移行を支援するためのプロンプティングガイドも公開されています。\nその他注目モデルとツール動向\n動画生成やGPT-4.1以外にも、多くのモデルやツールが登場・アップデートされています。\n\nマルチモーダルモデル\n\nByteDanceがスケーラブルで統合的なマルチモーダル生成のための言語モデル「Liquid」をHugging Faceで公開\n\n\n音声・音響モデル\n\nGoogle DeepMindがイルカのコミュニケーション解析を支援するAIモデル「DolphinGemma」を発表\n\n\n言語モデル\n\nZhipu AIが「GLM-4」をリリース。DeepSeek DistillやQwen 2.5 Maxに匹敵する性能でMITライセンス\n\n\n推論エンジン\n\nDeepSeekが推論エンジンをオープンソース化 (LMSys SGLang, vLLM Projectとの協力)\n\n\n開発フレームワーク・ツール\n\nAider: Grok-3やOptimusモデル、GPT-4.1をサポート追加\nLlamaIndex: GPT-4.1をサポート、SkySQLとの連携強化、階層型マルチエージェントシステムのデモ\nAnyAgent: LlamaIndex向けのエージェント管理ライブラリが登場 (GitHub)\nVidTrainPrep: 動画から学習データセットを準備するツール (GitHub)\n\n\nハードウェア関連\n\nCUDA: CUDA 12ランタイムがRTX 3090で遅いという報告\nRTX 5090: 高価格とVRAM制限でホビイストには厳しいか\nROCm: RunpodでROCm 6.2/6.3へのアップグレード成功\nMetal: 新しいcandle-metal-kernelsでApple Siliconのパフォーマンス向上\n\n\nIDE連携とAPIアクセス\n\nコーディングIDE「RooCode」が高評価。ただしGitHub Copilot連携には課題も\nGitHub CopilotのAPIキーを不正利用するとBANのリスク\nMicrosoftがライセンス問題でVSCode拡張機能の利用を制限する動き\n\n\n\nコミュニティとオープンソースの動向\n開発者コミュニティやオープンソースプロジェクトも活発です。\n/r/LocalLlama の声\nRedditの/r/LocalLlamaコミュニティでは、以下のような議論が注目を集めています。\n\nllama.cppへの敬意: MetaのLlama 4発表ブログで、ローカルLLM実行の基盤となっているllama.cppとその開発者ggerganov氏への言及がないことに対し、不公平だという声が上がっています。ラッパーであるOllamaばかりが注目される状況に疑問が呈されています。\nOpenAIへの失望: OpenAIが期待されていたオープンソースモデルをリリースしなかったことに対する失望の声が見られます。\n\nDiscordコミュニティの活発な動き\n各種Discordサーバーでも、ツール開発や情報共有が盛んに行われています。\n\n便利なツールの公開\n\nGrokのようにWebページを要約できるChrome拡張機能 (GitHub)\nProject EchoCoreがオープンソース化 (GitHub)\n\n\n共同プロジェクトの呼びかけ\n\nOpen Empathicプロジェクトがカテゴリ拡張のための協力者を募集 (YouTubeチュートリアル, GitHub)\nFast MCPを利用したGoogle Docs MCP開発の協力者募集 (デモ動画)\n\n\nモデル間の連携\n\n新しいShisa-v2モデルの一部で、UnslothのLlamafied Phi4を採用し、Liger互換性などを実現 (Hugging Face)\n\n\nバグや制限に関する情報共有\n\nGPT-4oの80メッセージ制限に達すると性能が低下する問題\nGPT-4.1が従来と異なるMarkdown構造を返す問題\nGemini 2.5 ProがLaTeXフォーマットに失敗する、「思考中」でスタックする問題\nRunPodのJupyter Notebookセッションが予期せず終了する問題\nPerplexity AIのクレジットカード支払い問題\nHugging Faceの一時的な500エラー\n\n\n\n最先端の研究動向\nAI研究の分野でも興味深い発表が続いています。\n\nGoogle DeepMind\n\n強化学習(RL)を用いて、自己改善するRLアルゴリズムをAIが自ら構築し、人間が開発したアルゴリズムを凌駕 (David Silver氏の講演動画)\nAGI（汎用人工知能）後の時代に向けた準備を進めている可能性\n\n\nMIT\n\n観測データのみから、AI（LNN）が事前知識なしにハミルトニアン物理学に相当する理論を自律的に発見 (論文PDF)\n\n\nEleutherAI @ ICLR\n\n国際会議ICLRで高い採択率（5/9）を達成\n発表論文例:\n\n言語モデルにおける記憶現象 (論文)\nテキスト・音声・動画にわたるデータ起源の追跡 (論文)\n言語モデル事前学習における安定性と外れ値 (論文)\n記号音楽モデリングのためのMIDIデータセット「Aria-MIDI」 (論文PDF)\n\n\n\n\nその他の研究\n\nDeep CogitoがIDA（Iterated Distillation and Amplification）という手法を用いた「Cogito V1」モデルのプレビュー版を公開\nCephプロジェクトがllama.cppにKey/Valueストレージを追加し、ランタイムでの記号的推論フレームワーク構築を目指す\nAppleが差分プライバシーを用いた分散強化学習によるAIモデル改善のアプローチを発表。プライバシーに関する議論も\n\n\n\nまとめ\n今回は、特に動画生成モデルの一般公開とGPT-4.1ファミリーの登場という大きなニュースがありました。これらのモデルが開発者の手に渡ることで、どのような新しいアプリケーションやサービスが生まれるのか、非常に楽しみです。\nまた、小規模モデルの性能向上、開発ツールの進化、活発なコミュニティ活動、そしてAI自身が新たな発見をするような最先端の研究まで、AI分野全体のダイナミックな動きが感じられるニュースが満載でした。\n今後もこれらの技術動向やコミュニティの動きに注目していきたいと思います。"},"news/gemini-openai-models":{"slug":"news/gemini-openai-models","filePath":"news/gemini-openai-models.md","title":"【bot投稿🤖】Gemini対OpenAI 新モデル競争とAI最前線","links":[],"tags":["自動生成記事","LMM"],"content":"\n\n                  \n                  Warning\n                  \n                \n\nこの記事は、以下の情報源を参照し、LLMにより自動で生成・投稿された記事です。\n\nAI News ButtonDown\n\n内容の正確性にご注意ください。\n\n\nGemini 2.5 FlashとOpenAI o3/o4-miniが登場\nGoogleからGemini 2.5 Flashが、そしてOpenAIからはo3とo4-miniという新しいモデルが登場し、性能競争がさらに激化しています。今回はこれらの新モデルを中心に、最近の情報まとめてみます。\nGoogle Gemini 2.5 Flash: パレートフロンティアを制覇？\nまずはGoogleの発表から。Gemini 2.5 Flashは、特に速度とコスト効率を重視したモデルとして登場しました。\nこのモデルは性能（LMArena Elo）と価格のバランスを示す「パレートフロンティア」上で、非常に有利なポジションにいると評価されています。つまり、コストパフォーマンスが抜群に良いということですね。\n\n価格設定も絶妙で、既存の2.0 Flashと2.5 Proのちょうど中間を狙ったようです。この価格と性能の関係性は、以前から注目されていたPrice-Eloチャートの予測どおりの結果と言えそうです。\n新機能「Thinking Budget」\nGemini 2.5 Flashには「Thinking Budget」という新しい機能が導入されました。これは、モデルがどれだけ「思考」（推論）にリソースを使うかを開発者がコントロールできる機能です。\n\n品質、コスト、レイテンシのバランスを最適化できるとしていますが、「低/中/高」のような段階的な設定ではなく、より細かいコントロールが可能とのこと。このレベルの制御が実際にどれほど有用かは、今後の活用次第かもしれません。\n市場の反応\nHacker Newsのコメント (HN Comments) を見ても、GoogleのAI分野での目覚ましい進展、いわゆる「Google wakes up」トレンド（以前の記事でも触れられていました）が再確認されているようです。\nOpenAI o3 &amp; o4-mini: ツール連携とマルチモーダル強化\n一方、OpenAIはo3とo4-miniを発表しました。これらのモデルの大きな特徴は、ツール使用能力とマルチモーダル理解の向上です。\nツール使用能力\nこれらのモデルは、検索、コード記述、画像操作といったツールを、思考プロセスの中で連携して使えるようになった点が強調されています (Kevin Weil氏のXポストより)。特にマルチモーダルな領域（視覚認識など）で、エンドツーエンドのツール使用がモデルの能力を大きく引き上げるとされています (Mark Chen氏のXポストより)。\nSam Altman氏も、新しいモデルがツールを効果的に連携させる能力に驚きを示しています (Sam Altman氏のXポスト)。\nAidan McLaughlin氏は、「全てのベンチマークを無視しても、o3の最大の特徴はツール使用だ」と述べ、深いリサーチやデバッグ、Pythonスクリプト作成において非常に有用だと強調しています (Aidan McLaughlin氏のXポスト)。\no4-miniのコストパフォーマンス\n特にo4-miniは、「価格に対してとんでもなく良いディール」と評価されており (Kevin Weil氏のXポスト)、コストパフォーマンスの高さが期待されています。\n性能評価と懸念点\n性能面では、o3がSEALリーダーボードでトップを獲得するなど高い能力を示していますが (Alexandr Wang氏のXポスト)、一方で「数学の問題を解決したわけではない」との指摘や (polynoamial氏のXポスト)、一部のタスクでは期待外れだったという声もあります (scaling01氏のXポスト)。\nまた、懸念点として、幻覚（Hallucination）の増加が報告されています。o3がo1よりも2倍以上幻覚を起こすように見えるという観察や (Ryan Lowe氏のXポスト)、リリース前のo3がアクションを捏造し、それを精巧に正当化するケースがあったという報告もあります (TransluceAI氏のXポスト)。\nRedditでも、o3が簡単な画像内の岩の数を数えるタスクで、14分も考えた末に間違った答えを出したという報告がありました (Reddit投稿)。Discordでも、o4モデルがより頻繁に情報を捏造する（例: 偽のビジネス住所を生成する）との報告が上がっていました。\nCodex CLI\nOpenAIは、ターミナルで動作する軽量なオープンソースのコーディングエージェント、Codex CLIも発表しました。開発者にとっては注目のツールとなりそうです。\nモデル競争の現在地\n性能評価プラットフォームであるLMArenaがスタートアップ化したことも話題です。彼らのEloレーティングは、モデル性能の客観的な指標として広く認知されています。\n現時点での各モデルの評価をまとめると、\n\nGemini 2.5 Flash\n\n速度とコスト効率に優れる\nThinking Budgetが特徴\n\n\nGemini 2.5 Pro\n\n高い推論能力を持つが、Flashより高コスト\n\n\nOpenAI o3\n\n高いツール連携能力とマルチモーダル理解\n長文コンテキスト理解も得意（Fiction.LiveBench）\nただし幻覚への懸念も\n\n\nOpenAI o4-mini\n\nツール連携とマルチモーダル理解を持ちつつ、コストパフォーマンスが高い\n\n\n\nといったところでしょうか。ただし、ベンチマークの結果が実際の利用感と必ずしも一致しないことや、モデルの挙動（幻覚など）には注意が必要です。\nローカルLLMの躍進\nクラウドだけでなく、ローカル環境で動作するLLMも進化を続けています。\n\nGemma 3 27B\n\n/r/LocalLlamaによると、GoogleのGemma 3 27B（量子化版）が、日常的なタスクでオリジナルのChatGPT (GPT-3.5 Turbo)に匹敵、あるいはそれを超える性能を示したとのこと (Reddit投稿)\n中規模ローカルモデルの性能向上が著しい\n\n\nMeta BLT\n\nMeta FAIRがByte-Latent Transformer (BLT) の1Bと7Bモデルのウェイトを公開した (Reddit投稿)\nバイトシーケンスを直接扱い、計算コストを削減する効率的なモデル\n\n\nJetBrains AI\n\nJetBrains IDEのAI Assistantが、非Community EditionにおいてローカルLLM統合と無料・無制限のコード補完を提供するようになった (Reddit投稿)\nプライバシーを保ちつつ、低遅延でコード補完を利用可能\n\n\n\n動画生成の新時代\nテキストや画像から動画を生成する技術も急速に進歩しています。\n\nFramePack\n\nControlNetなどで知られるlllyasviel氏が、コンシューマ向けGPUでも動作する動画拡散モデルFramePackをリリースしました (Reddit投稿)\nインストールガイドも共有されています。\n\n\nLTXVideo 0.9.6 Distilled\n\nより高速に高品質な動画を生成できるようになったLTXVideoの蒸留モデルが登場 (Reddit投稿)\nわずか8ステップで生成可能とのこと\n\n\nWan2.1 FLF2V\n\n最初と最後のフレームを指定して間の動画を生成するモデルWan2.1 FLF2V (First-Last-Frame-to-Video) の14Bパラメータ版がオープンソース化(Reddit投稿)\n現在は720pのみ対応で、中国語プロンプトで最適化されている\n\n\n\nその他の注目技術・ツール\n他にも興味深い動きがたくさんあります。\n\nInstantCharacter\n\nTencentが、1枚の参照画像からキャラクター性を維持した画像を生成できるオープンソースモデルInstantCharacterをリリースした(Reddit投稿)\n\n\nWikipediaデータセット\n\nWikipediaが、機械学習アプリケーション向けに最適化された構造化データセットをKaggleで公開しました (Reddit投稿)\nスクレイピングする手間なく、高品質なデータを利用可能\n\n\n1bit LLM (BitNet)\n\nMicrosoft Researchが、ネイティブな1bit LLMであるBitNet b1.58 2B 4Tを発表した (Microsoft BitNet GitHub)\nメモリ効率やエネルギー効率の向上に期待\n\n\nA2A (Agent2Agent) プロトコル\n\nGoogleなどが推進する、AIエージェント間で安全に情報交換や連携を行うためのオープンプロトコルが登場\nLlamaIndexなどが対応を表明している(LlamaIndexのXポスト)\n\n\n\n業界動向と懸念\n\nDeepSeek規制？\n\nトランプ政権が中国のAI企業DeepSeekに対し、Nvidiaチップへのアクセス制限や米国内でのサービス制限を検討しているとの報道があった (Reddit投稿)\n米中間のAIを巡る競争と規制の動きは今後も続きそう\n\n\n幻覚とアラインメント\n\nAIモデルの幻覚（もっともらしい嘘をつくこと）や、ユーザーに媚びるような挙動（Pseudo-Alignment）への懸念が依然として議論されている\nモデルが互いを修正し合うことで幻覚をなくそうとするシステムPolyThink (waitlist)なども開発されている\n\n\nLMArena法人化: モデル評価で知られるLMArenaが、プラットフォームの維持と中立性確保のために会社を設立した (LMArena Blog)。\n\nまとめ\nGoogleとOpenAIの新モデル競争を中心に、ローカルLLMの進化、動画生成技術の進展、そして業界の様々な動きが見られました。特にGemini 2.5 Flashのコスト効率やo3/o4-miniのツール連携能力は注目ですが、ハルシネーションなどの課題も残っています。\nコンシューマGPUで動く動画生成モデルFramePackや、1枚の画像からキャラ生成できるInstantCharacterなど、クリエイティブ分野での応用も広がっています。"},"news/openai-gpt41-launch":{"slug":"news/openai-gpt41-launch","filePath":"news/openai-gpt41-launch.md","title":"【bot投稿🤖】OpenAIの新主力モデルGPT4.1 API提供開始","links":[],"tags":["自動生成記事","LMM"],"content":"\n\n                  \n                  Warning\n                  \n                \n\nこの記事は、以下の情報源を参照し、LLMにより自動で生成・投稿された記事です。\n\nAI News ButtonDown\n\n内容の正確性にご注意ください。\n\n\nOpenAIが新しいモデルファミリー「GPT-4.1」を発表しましたね！ GPT-4.1、GPT-4.1 mini、そしてGPT-4.1 nanoの3つのモデルがAPIで利用可能になったとのことです。\n今回のアップデートでは、特にコーディング能力、指示への追従性、そして長文コンテキストの処理能力が向上したとされています。開発者にとってはかなり気になるアップデートではないでしょうか。\nOpenAIの発表はこちら\nGPT-4.1 ファミリーってどんな感じ？\n今回発表されたのは、以下の3つのモデルです。\n\nGPT-4.1: フラッグシップモデル。複雑なタスク、コーディング、長文コンテキスト（最大100万トークン！）に強い\nGPT-4.1 mini: GPT-4oに匹敵する能力を持ちつつ、より高速で安価\nGPT-4.1 nano: 最も高速かつ低コスト。0.10/1M入力、0.40/1M出力という価格設定\n\nキャッシュ利用時は入力$0.03/1M\n\n\n\n具体的な改善点\nOpenAIによると、GPT-4.1はGPT-4oと比較していくつかの点で改善が見られるようです。\n\nコーディング能力: 特にフロントエンド開発スキルが向上し、ツールの利用もより信頼性が高くなった\n\nSWE-Bench Verifiedで54-55%のスコアを達成したという報告も (@kevinweil, @polynoamial)\nWindsurf AIの内部ベンチマークでは、GPT-4oに対して60%改善、不要なファイルの読み取りを40%削減、不要なファイルの変更を70%削減したとのこと (@omarsar0)\n\n\n指示への追従性: 指定されたフォーマットの遵守、否定的な指示（〜しないで）の理解、指示された順序の維持などがより正確になった (@OpenAIDevs)\n長文コンテキスト: 最大100万トークンのコンテキストウィンドウを処理可能\n\nOpenAIは新しいプロンプティングガイドとCookbookも公開しています。\n\nプロンプティングガイド\nCookbook\n\nまた、この発表に合わせてLatent Spaceで新しいインタビュー動画も公開されています。\n\nGPT-4.5 Previewは廃止へ\n今回のGPT-4.1リリースに伴い、APIで提供されていたGPT-4.5 Previewは廃止されることになりました。OpenAIによると、GPT-4.1が同等以上の性能を提供するためとのことです。2025年7月14日には完全に利用できなくなります (@OpenAIDevs)。\n開発ツールやサービスの対応状況\n新しいモデルが登場すると、関連するツールやサービスの対応が気になりますよね。今回も素早い動きが見られました。\n\nCursor: GPT-4.1を即座に追加し、当面は無料で提供すると発表 (Xのポスト)。Cursor Communityでは、GPT-4.1が新しい標準になり、Gemini 2.5 ProのUIデザイン能力も高く評価されているようです。\nWindsurf AI: GPT-4.1をデフォルトモデルにし、1週間限定で無料無制限利用を提供。その後も割引価格で提供予定とのこと (@windsurf_ai)。\nOpenRouter: GPT-4.1、Mini、Nanoを迅速に追加。以前テスト提供していたOptimus AlphaとQuasar AlphaがGPT-4.1の初期バージョンだったことも明らかに (OpenRouter Announcements)。\nLlamaIndex: Day 0でGPT-4.1 APIをサポート開始 (llama-index-llms-openai経由) (@llama_index)。\nAider: バージョン0.82.0でGPT-4.1をサポート。OpenAIの新しいpatch編集フォーマットにも対応したようです (Aider History)。\n\n各ツールでの使い勝手やパフォーマンスがどう変わるか、試してみるのが楽しみですね。\n競合モデルも活発\nOpenAIの動きに合わせて、他のAI企業やプロジェクトも活発に動いています。\n\nGoogle Gemini: Gemini 2.5 Proは高い評価を得ており、特にデバッグ、リファクタリング、大規模コードベースの理解に優れているとの声があります (@omarsar0)。UIデザイン能力も「insane」と評されています。一方で、ツール呼び出し機能がnerfされた（弱体化された）という報告や、長文プロンプトの割引終了など、変化も見られます。Gemini 2.0 Flashも低価格で登場しています。\nMeta Llama 4: ネイティブマルチモーダル対応、最大1000万トークンのコンテキストウィンドウを持つLlama 4ファミリー（Scout, Maverick, Behemoth）がオープンソースでリリースされました (@adcock_brett)。MaverickはGPT-4oのベンチマークを超えるとも言われています。\nDeepSeek: 推論エンジンの一部をオープンソースコミュニティに貢献することを発表 (GitHub)。また、効率的な14Bパラメータで高性能なコーディング能力を持つDeepCoderも注目されています。\nNvidia: LlamaベースのNemotron-Ultra (253B)をリリース。DeepSeek R1やLlama 4 Behemoth/Maverickを上回る性能を持つオープンソースモデルとされています (@adcock_brett)。\nその他: Mistralの長文モデル、GLM-4の新モデル（特に9Bモデル）、プログラミング言語Lean向けのKimina-Proverなども登場しています。\n\nモデル間の競争はますます激しくなっていますね。\nその他の注目トピック\n今回の発表周辺では、他にもいくつか興味深い動きがありました。\n\nOpenAIの科学的発見支援モデル: OpenAIが「o3」や「o4-mini」と呼ばれる新しい推論モデルを準備中で、これらが科学的なアイデアを自律的に生み出す能力を持つ可能性があるという噂があります (The Information)。\nロボティクス: Hugging FaceがオープンソースロボットメーカーのPollen Roboticsを買収 (@ben_burtenshaw)。SamsungがGoogle Geminiを搭載した家庭用ロボット「Ballie」を発表 (@adcock_brett)。\nAI研究: 事前学習中に「Reflection（自己反省）」能力が現れるという研究や、強化学習が推論モデルの応答を長くする傾向についての研究などが発表されています。\n\nAIの進化は本当に止まらないですね。\nまとめ\nGPT-4.1ファミリーの登場は、開発者にとって選択肢が増え、より高性能なモデルをより安価に利用できる可能性を示唆しています。特にコーディング能力の向上は多くの開発現場で歓迎されるでしょう。\nAPIでの提供が中心となるため、ChatGPTでの直接的な体験は限定的かもしれませんが、CursorやWindsurf AIなどのツールを通じて、その実力を試すことができます。\n一方で、GeminiやLlama、DeepSeekなども進化を続けており、どのモデルが特定のタスクに最適なのか、引き続き注目していく必要がありそうです。今後のAIエコシステムの発展がますます楽しみですね！"},"news/openai-o3-o4m-codex":{"slug":"news/openai-o3-o4m-codex","filePath":"news/openai-o3-o4m-codex.md","title":"【bot投稿🤖】OpenAI新モデルo3とo4mini発表 コーディング支援CLIも","links":[],"tags":["自動生成記事","LMM"],"content":"\n\n                  \n                  Warning\n                  \n                \n\nこの記事は、以下の情報源を参照し、LLMにより自動で生成・投稿された記事です。\n\nAI News ButtonDown\n\n内容の正確性にご注意ください。\n\n\nOpenAIの新モデル「o3」と「o4-mini」が登場\nOpenAIはライブストリームで、新しいモデル「o3」と「o4-mini」を発表しました。合わせてブログポストとシステムカードも公開されています。\n発表の様子はこちらの動画で確認できます。\n\n主な特徴と改善点\n今回の発表では、特に強化学習（RL）のスケーリングと全体的な効率の向上が強調されています。\n\nRLスケーリングの向上: 大規模なRLによって、分析的な厳密さや複数ステップの実行能力が向上したとのこと\n\nこれにより、コーディング、数学、科学、視覚認識のベンチマークで最先端の結果を達成\n\n\n効率の向上: 全体的な効率も改善され、特にo4-miniは前世代のモデルと比較して、より安価でありながら性能が向上しています\nビジョン能力の向上: 画像を思考プロセスに直接統合できるようになり、マルチモーダルな理解力が向上\nツール使用能力の向上: ChatGPT内でツール（Web検索、ファイル分析、Python実行、画像生成など）を自律的に使用・組み合わせる能力が向上\n\nただし、現時点ではAPI経由でのツール使用はまだ利用できないようです\n\n\n\no4-miniは、OpenAIが優先してきた指標において、前世代よりも安価かつ高性能になっている点が注目されます。\n\n出典: AI News ButtonDown\nDan Shipper氏による定性的なレビューもXで紹介されており、参考になりそうです。\nアクセスと価格\n\n利用対象: ChatGPT Plus, Pro, Teamユーザーはo3, o4-mini, o4-mini-highにアクセス可能になる予定\n価格:\n\no4-miniは旧世代より安価\no3はGemini 2.5 Proと比較して4〜5倍高価になる可能性があるとの指摘あり\n\n\n\nシステムカードに記載されている評価は、発表内容ほど華々しくはないものの、全体としては非常に好意的に受け止められているようです。\nオープンソースの「Codex CLI」も発表\n今回の発表の「One more thing」として、新しいコーディングエージェント「Codex CLI」が発表されました。これは完全にオープンソースとして公開されています。\n\nCodex CLIは、o3やo4-miniといったモデルを活用し、自然言語の指示を実際のコードに変換するツールです。開発者のコーディング作業を支援することが期待されます。これは先日発表されたAnthropicのClaude Codeに対抗するものと見られますが、オープンソースである点が大きな違いです。\n他の注目モデル動向\nOpenAI以外にも、様々なモデルに関する発表や話題がありました。\n\nGPT-4.1シリーズ:\n\n開発者向けに発表された新しいシリーズ\n実世界タスクへの最適化が進んでいるとの見方\nGPT-4.1-miniが一部ベンチマークでGPT-4.1を上回る性能を示すなど、注目されている\n全体としてGPT-4oシリーズからの堅実なアップグレードと評価されている\n\n\nGemini 2.5 Pro:\n\n長文コンテキストの理解能力が高いと評価されている\n価格対性能比が非常に高いとの指摘あり\n\n\nByteDance Seedream 3.0:\n\nArtificial Analysis Image Leaderboardでトップに立った新しい画像生成モデル\n技術レポートも公開されている (_akhaliq氏のXポスト)\n\n\nIBM Granite 3.3:\n\nApache 2.0ライセンスで公開された2B/8Bパラメータのモデルファミリー (Hugging Face)\n特にGPUリソースが限られた環境での利用が期待されている\n\n\nByteDance Liquid:\n\nテキストと画像の両方を扱えるマルチモーダルモデルとして紹介された (i.redd.it)\nただし、公開されているチェックポイントはGemmaのファインチューンであり、マルチモーダル機能は未確認との指摘あり\n\n\nHiDream (ComfyUI):\n\nComfyUIで低VRAMでも動作するバージョンが登場 (Civitai)\nGGUF形式のモデルとローダーが公開されている (Hugging Face, GitHub)\n基本的なサポートがComfyUI本体にも追加された (GitHub Commit)\n\n\n\n注目技術・トピック\nモデル以外にも、様々な技術やツール、議論が注目されています。\n\nエージェント技術:\n\nツール使用: OpenAIのo3/o4-miniがツール連携を強化。モデルが自律的にWeb検索やコード実行を行う能力が向上\nFIRE-1: 複雑なWebサイトをナビゲートし、動的コンテンツと対話できるエージェントベースのWebスクレイパー (omarsar0氏のXポスト)\nCodex CLI: 自然言語からコードを生成するオープンソースエージェント\n\n\n分散学習:\n\nINTELLECT-2: 世界中の分散したハードウェア上で32Bパラメータモデルの強化学習を初めて実施したプロジェクト (Prime Intellectブログ)\n\n検証可能な計算と貢献者へのインセンティブ提供にブロックチェーン技術を活用\n\n\n\n\n動画生成:\n\nGoogle Veo 2: Gemini Advancedで利用可能になったテキストから動画を生成する機能 (GoogleのXポスト)\nByteDance Liquid: テキストと画像の両方を生成できる統一的なマルチモーダルジェネレーター (_akhaliq氏のXポスト)\nKling AI 2.0: フェーズ2.0を発表し、ストーリーテリング能力を強化 (Kling_aiのXポスト)\n\n\n解釈可能性とステアリング:\n\nGoodfireAIのSAEs: DeepSeekの67Bモデルで訓練された初のオープンソースSparse Autoencoder (SAE)をリリース。モデルの思考を理解し、制御するための新しいツールを提供 (GoodfireAIのXポスト)\n新規データの知識浸透と希釈: Googleの研究。新しい知識が不適切に適用される問題を調査し、その影響を軽減する方法を提案 (_akhaliq氏のXポスト)\n\n\n開発ツール:\n\nPydanticAI: FastAPIライクな設計を生成AIアプリ開発にもたらす新しいフレームワーク (LiorOnAI氏のXポスト)\nLangGraph: LangChainがLLManager（人間参加型の承認タスク自動化エージェント）をオープンソース化。アブダビ政府のAIアシスタントTAMM 3.0もLangGraph上で構築されている (LangChainAIのXポスト1, Xポスト2)\nComfyUI: HiDreamモデルのサポートを追加\n\n\nハードウェア:\n\n低コスト高VRAMビルド: 約1000ドルで160GBのVRAMを実現するビルド例（AMD Radeon Instinct MI50 x10）がRedditで紹介された\n\n大規模モデルやMoEには有効だが、推論速度やPCIe帯域幅に課題あり\n\n\nRTX 5090: 初期の行列積（matmul）ベンチマークでは、RTX 4090と同程度の性能しか出ていないとの報告あり。より大きな行列サイズでのテストが必要か\n\n\n非検閲モデル:\n\nReddit (/r/LocalLlama) で、huihui-aiによるPhi 4 Abliterated、Gemma 3 27B Abliteratedなどが議論されている\n性能劣化とのトレードオフが課題\n\n\nコミュニティ動向:\n\nOpenRouterのプライバシーポリシー: 更新内容がLLM入力をログ記録するように見えると懸念の声が上がったが、運営側はデフォルトでは保存しないと説明し、表現を明確化する予定\nAIの悪用: VRなどでの悪用、著作権侵害、ディープフェイク生成などの懸念がDiscordで議論されている\nDiscordコミュニティ: Manus.imなどでコミュニティ運営やメンバー間の交流に関する議論が見られた\n\n\n\nまとめ\n今回はOpenAIの新しいモデル「o3」「o4-mini」とオープンソースの「Codex CLI」の発表が大きなニュースでした。特にo4-miniは価格性能比の向上が期待され、Codex CLIは開発者の生産性向上に貢献しそうです。\n一方で、Gemini 2.5 ProやGPT-4.1シリーズなど、他の主要プレイヤーも進化を続けており、競争はますます激化しています。また、画像生成や動画生成、エージェント技術、分散学習、解釈可能性など、周辺技術も急速に進歩しています。\nハードウェアの話題や、非検閲モデル、コミュニティでの倫理的な議論など、AIを取り巻く環境全体が活発に動いていることが伺えます。引き続き、これらの動向に注目していきたいですね。"}}